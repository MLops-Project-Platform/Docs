"use strict";(self.webpackChunkmlops_documentation=self.webpackChunkmlops_documentation||[]).push([[1780],{8377:function(e,n,i){i.r(n),i.d(n,{assets:function(){return c},contentTitle:function(){return t},default:function(){return u},frontMatter:function(){return a},metadata:function(){return r},toc:function(){return o}});var r=JSON.parse('{"id":"tools/gpu-scheduling","title":"NVIDIA GPU Scheduling","description":"Overview","source":"@site/docs/tools/gpu-scheduling.md","sourceDirName":"tools","slug":"/tools/gpu-scheduling","permalink":"/docs/tools/gpu-scheduling","draft":false,"unlisted":false,"editUrl":"https://github.com/MLops-Project-Platform/documentation/tree/main/docs/tools/gpu-scheduling.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"ArgoCD - GitOps for Kubernetes","permalink":"/docs/tools/argocd"},"next":{"title":"MinIO (S3 Storage)","permalink":"/docs/tools/minio"}}'),s=i(4848),l=i(8453);const a={sidebar_position:8},t="NVIDIA GPU Scheduling",c={},o=[{value:"Overview",id:"overview",level:2},{value:"GPU Types",id:"gpu-types",level:2},{value:"Full GPU Access",id:"full-gpu-access",level:3},{value:"Fractional GPU (MIG - Multi-Instance GPU)",id:"fractional-gpu-mig---multi-instance-gpu",level:3},{value:"GPU Device Plugin Installation",id:"gpu-device-plugin-installation",level:2},{value:"Install NVIDIA Device Plugin",id:"install-nvidia-device-plugin",level:3},{value:"Check GPU Status",id:"check-gpu-status",level:3},{value:"MIG (Multi-Instance GPU) Setup",id:"mig-multi-instance-gpu-setup",level:2},{value:"Enable MIG Mode",id:"enable-mig-mode",level:3},{value:"MIG GPU Resource Mapping",id:"mig-gpu-resource-mapping",level:3},{value:"GPU Resource Requests",id:"gpu-resource-requests",level:2},{value:"Training Jobs (Full GPU)",id:"training-jobs-full-gpu",level:3},{value:"Inference (Fractional GPU)",id:"inference-fractional-gpu",level:3},{value:"Multi-GPU Training",id:"multi-gpu-training",level:3},{value:"GPU Affinity and Node Selection",id:"gpu-affinity-and-node-selection",level:2},{value:"Request Specific GPU Type",id:"request-specific-gpu-type",level:3},{value:"Pod Affinity (Collocate on same node)",id:"pod-affinity-collocate-on-same-node",level:3},{value:"Pod Anti-Affinity (Spread across nodes)",id:"pod-anti-affinity-spread-across-nodes",level:3},{value:"GPU Monitoring",id:"gpu-monitoring",level:2},{value:"Monitor GPU Usage",id:"monitor-gpu-usage",level:3},{value:"Prometheus GPU Metrics",id:"prometheus-gpu-metrics",level:3},{value:"Check GPU Process Info",id:"check-gpu-process-info",level:3},{value:"CUDA and cuDNN Setup",id:"cuda-and-cudnn-setup",level:2},{value:"Base Docker Image with CUDA",id:"base-docker-image-with-cuda",level:3},{value:"Verify CUDA in Container",id:"verify-cuda-in-container",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"GPU not detected",id:"gpu-not-detected",level:3},{value:"GPU allocation errors",id:"gpu-allocation-errors",level:3},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"nvidia-gpu-scheduling",children:"NVIDIA GPU Scheduling"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"NVIDIA GPU scheduling in Kubernetes enables efficient allocation and utilization of GPU resources across training workloads and inference services."}),"\n",(0,s.jsx)(n.h2,{id:"gpu-types",children:"GPU Types"}),"\n",(0,s.jsx)(n.h3,{id:"full-gpu-access",children:"Full GPU Access"}),"\n",(0,s.jsx)(n.p,{children:"Allocate entire GPU to a single pod:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'apiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: training-job-full-gpu\r\nspec:\r\n  containers:\r\n  - name: trainer\r\n    image: mlops/trainer:cuda-11.8\r\n    resources:\r\n      limits:\r\n        nvidia.com/gpu: 1  # Request 1 full GPU\r\n    env:\r\n    - name: CUDA_VISIBLE_DEVICES\r\n      value: "0"\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Use Case:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Large-scale model training"}),"\n",(0,s.jsx)(n.li,{children:"Memory-intensive workloads"}),"\n",(0,s.jsx)(n.li,{children:"Minimal context switching needed"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"fractional-gpu-mig---multi-instance-gpu",children:"Fractional GPU (MIG - Multi-Instance GPU)"}),"\n",(0,s.jsx)(n.p,{children:"Share a single GPU across multiple pods:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'apiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: inference-fractional-gpu\r\nspec:\r\n  containers:\r\n  - name: server\r\n    image: mlops/inference:latest\r\n    resources:\r\n      limits:\r\n        nvidia.com/gpu: "0.5"  # Request 50% of GPU\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Supported on:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"NVIDIA A100, A30, A10G"}),"\n",(0,s.jsx)(n.li,{children:"Requires NVIDIA MIG mode enabled"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Benefits:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Cost efficiency for inference"}),"\n",(0,s.jsx)(n.li,{children:"Better resource utilization"}),"\n",(0,s.jsx)(n.li,{children:"Multiple models per GPU"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Not supported on older GPUs"}),"\n",(0,s.jsx)(n.li,{children:"Requires MIG partitioning setup"}),"\n",(0,s.jsx)(n.li,{children:"Lower performance than full GPU"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"gpu-device-plugin-installation",children:"GPU Device Plugin Installation"}),"\n",(0,s.jsx)(n.h3,{id:"install-nvidia-device-plugin",children:"Install NVIDIA Device Plugin"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install NVIDIA Device Plugin\r\nkubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.0/nvidia-device-plugin.yml\r\n\r\n# Verify installation\r\nkubectl get nodes -L nvidia.com/gpu\r\n\r\n# Check available GPUs\r\nkubectl describe node <node-name> | grep nvidia.com/gpu\n"})}),"\n",(0,s.jsx)(n.h3,{id:"check-gpu-status",children:"Check GPU Status"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# View GPU allocation\r\nkubectl get nodes -o custom-columns=NAME:.metadata.name,GPUS:.status.allocatable.nvidia\\\\.com/gpu\r\n\r\n# Detailed GPU info\r\nkubectl describe node gpu-node-1 | grep -A 5 "nvidia.com"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"mig-multi-instance-gpu-setup",children:"MIG (Multi-Instance GPU) Setup"}),"\n",(0,s.jsx)(n.h3,{id:"enable-mig-mode",children:"Enable MIG Mode"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# SSH to GPU node\r\nssh user@gpu-node\r\n\r\n# Enable MIG mode (requires root)\r\nsudo nvidia-smi -mig 1\r\n\r\n# Verify MIG is enabled\r\nnvidia-smi -L\r\n\r\n# Create MIG profiles\r\nsudo nvidia-smi -mig c.1g.6gb,c.1g.6gb,c.1g.6gb  # 3x 1g.6gb profiles\n"})}),"\n",(0,s.jsx)(n.h3,{id:"mig-gpu-resource-mapping",children:"MIG GPU Resource Mapping"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# Define MIG resources in device plugin config\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: nvidia-device-plugin-configs\r\n  namespace: kube-system\r\ndata:\r\n  any: |\r\n    version: v1\r\n    sharing:\r\n      timeSlicing:\r\n        replicas: 4\r\n      mig:\r\n        strategy: mixed\r\n        devices:\r\n          all: "1g.6gb"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"gpu-resource-requests",children:"GPU Resource Requests"}),"\n",(0,s.jsx)(n.h3,{id:"training-jobs-full-gpu",children:"Training Jobs (Full GPU)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'apiVersion: batch/v1\r\nkind: Job\r\nmetadata:\r\n  name: bert-training\r\nspec:\r\n  template:\r\n    spec:\r\n      containers:\r\n      - name: trainer\r\n        image: mlops/trainer:cuda-11.8\r\n        command: ["python", "train.py"]\r\n        resources:\r\n          requests:\r\n            nvidia.com/gpu: 1\r\n            memory: "16Gi"\r\n            cpu: "8"\r\n          limits:\r\n            nvidia.com/gpu: 1\r\n            memory: "16Gi"\r\n            cpu: "8"\r\n      restartPolicy: Never\r\n  backoffLimit: 3\n'})}),"\n",(0,s.jsx)(n.h3,{id:"inference-fractional-gpu",children:"Inference (Fractional GPU)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'apiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: model-serving\r\nspec:\r\n  replicas: 3\r\n  selector:\r\n    matchLabels:\r\n      app: model-server\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: model-server\r\n    spec:\r\n      containers:\r\n      - name: inference\r\n        image: mlops/inference:latest\r\n        resources:\r\n          requests:\r\n            nvidia.com/gpu: "0.25"  # 25% of GPU\r\n            memory: "4Gi"\r\n            cpu: "2"\r\n          limits:\r\n            nvidia.com/gpu: "0.25"\r\n            memory: "4Gi"\r\n            cpu: "2"\r\n      affinity:\r\n        podAntiAffinity:\r\n          preferredDuringSchedulingIgnoredDuringExecution:\r\n          - weight: 100\r\n            podAffinityTerm:\r\n              labelSelector:\r\n                matchExpressions:\r\n                - key: app\r\n                  operator: In\r\n                  values:\r\n                  - model-server\r\n              topologyKey: kubernetes.io/hostname\n'})}),"\n",(0,s.jsx)(n.h3,{id:"multi-gpu-training",children:"Multi-GPU Training"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'apiVersion: batch/v1\r\nkind: Job\r\nmetadata:\r\n  name: distributed-training\r\nspec:\r\n  template:\r\n    spec:\r\n      containers:\r\n      - name: trainer\r\n        image: mlops/trainer:cuda-11.8\r\n        env:\r\n        - name: NVIDIA_VISIBLE_DEVICES\r\n          value: "all"\r\n        - name: CUDA_VISIBLE_DEVICES\r\n          value: "0,1"  # Use GPUs 0 and 1\r\n        resources:\r\n          requests:\r\n            nvidia.com/gpu: 2\r\n            memory: "32Gi"\r\n            cpu: "16"\r\n          limits:\r\n            nvidia.com/gpu: 2\r\n            memory: "32Gi"\r\n            cpu: "16"\r\n      restartPolicy: Never\n'})}),"\n",(0,s.jsx)(n.h2,{id:"gpu-affinity-and-node-selection",children:"GPU Affinity and Node Selection"}),"\n",(0,s.jsx)(n.h3,{id:"request-specific-gpu-type",children:"Request Specific GPU Type"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'spec:\r\n  nodeSelector:\r\n    nvidia.com/gpu-memory: "40gb"  # Request A100 or similar\r\n  containers:\r\n  - name: trainer\r\n    resources:\r\n      limits:\r\n        nvidia.com/gpu: 1\n'})}),"\n",(0,s.jsx)(n.h3,{id:"pod-affinity-collocate-on-same-node",children:"Pod Affinity (Collocate on same node)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"spec:\r\n  affinity:\r\n    podAffinity:\r\n      requiredDuringSchedulingIgnoredDuringExecution:\r\n      - labelSelector:\r\n          matchExpressions:\r\n          - key: app\r\n            operator: In\r\n            values:\r\n            - training-worker\r\n        topologyKey: kubernetes.io/hostname\n"})}),"\n",(0,s.jsx)(n.h3,{id:"pod-anti-affinity-spread-across-nodes",children:"Pod Anti-Affinity (Spread across nodes)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"spec:\r\n  affinity:\r\n    podAntiAffinity:\r\n      preferredDuringSchedulingIgnoredDuringExecution:\r\n      - weight: 100\r\n        podAffinityTerm:\r\n          labelSelector:\r\n            matchExpressions:\r\n            - key: app\r\n              operator: In\r\n              values:\r\n              - model-server\r\n          topologyKey: kubernetes.io/hostname\n"})}),"\n",(0,s.jsx)(n.h2,{id:"gpu-monitoring",children:"GPU Monitoring"}),"\n",(0,s.jsx)(n.h3,{id:"monitor-gpu-usage",children:"Monitor GPU Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# SSH to GPU node\r\nssh user@gpu-node\r\n\r\n# Real-time GPU monitoring\r\nwatch -n 1 nvidia-smi\r\n\r\n# Monitor in container\r\nkubectl exec -it pod-name -- nvidia-smi\n"})}),"\n",(0,s.jsx)(n.h3,{id:"prometheus-gpu-metrics",children:"Prometheus GPU Metrics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"apiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: gpu-exporter\r\n  namespace: monitoring\r\nspec:\r\n  selector:\r\n    app: gpu-exporter\r\n  ports:\r\n  - port: 9400\r\n    name: metrics\n"})}),"\n",(0,s.jsx)(n.h3,{id:"check-gpu-process-info",children:"Check GPU Process Info"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"nvidia-smi --query-compute-apps=pid,process_name,gpu_memory_usage --format=csv\r\n\r\nnvidia-smi pmon -c 1  # Show process monitoring\n"})}),"\n",(0,s.jsx)(n.h2,{id:"cuda-and-cudnn-setup",children:"CUDA and cuDNN Setup"}),"\n",(0,s.jsx)(n.h3,{id:"base-docker-image-with-cuda",children:"Base Docker Image with CUDA"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:'FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04\r\n\r\n# Install dependencies\r\nRUN apt-get update && apt-get install -y \\\r\n    python3.10 \\\r\n    python3-pip \\\r\n    && rm -rf /var/lib/apt/lists/*\r\n\r\n# Install PyTorch with CUDA support\r\nRUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\r\n\r\n# Install TensorFlow with CUDA support\r\nRUN pip install tensorflow[and-cuda]\r\n\r\nWORKDIR /app\r\nCOPY . .\r\n\r\nENTRYPOINT ["python3", "train.py"]\n'})}),"\n",(0,s.jsx)(n.h3,{id:"verify-cuda-in-container",children:"Verify CUDA in Container"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Check CUDA availability\r\npython -c "import torch; print(torch.cuda.is_available())"\r\n\r\n# List visible GPUs\r\npython -c "import torch; print(torch.cuda.device_count())"\r\n\r\n# Check CUDA version\r\nnvidia-smi\n'})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Always specify resource limits"})," - Prevent GPU over-allocation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use MIG for inference"})," - Cost-effective serving"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use full GPUs for training"})," - Better performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitor GPU utilization"})," - Track efficiency"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Set appropriate timeouts"})," - Prevent hanging jobs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use node selectors"})," - Target specific GPU types"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement pod affinity"})," - Optimize placement"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-not-detected",children:"GPU not detected"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check device plugin status\r\nkubectl get daemonset -n kube-system | grep gpu\r\n\r\n# Check logs\r\nkubectl logs -n kube-system -l k8s-app=nvidia-device-plugin\r\n\r\n# Verify device plugin is running\r\nkubectl describe daemonset nvidia-device-plugin -n kube-system\n"})}),"\n",(0,s.jsx)(n.h3,{id:"gpu-allocation-errors",children:"GPU allocation errors"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check node GPU resources\r\nkubectl describe node gpu-node-1 | grep nvidia.com\r\n\r\n# Check pod events\r\nkubectl describe pod training-job | grep -A 5 Events\r\n\r\n# Check for pending pods\r\nkubectl get pods -o wide | grep Pending\n"})}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/NVIDIA/k8s-device-plugin",children:"NVIDIA Device Plugin"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/tesla/mig-user-guide/",children:"NVIDIA MIG User Guide"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/cuda/cuda-installation-guide-linux/",children:"CUDA Installation Guide"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:function(e,n,i){i.d(n,{R:function(){return a},x:function(){return t}});var r=i(6540);const s={},l=r.createContext(s);function a(e){const n=r.useContext(l);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);