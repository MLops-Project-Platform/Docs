"use strict";(self.webpackChunkmlops_documentation=self.webpackChunkmlops_documentation||[]).push([[9162],{6169:function(e,r,n){n.r(r),n.d(r,{assets:function(){return l},contentTitle:function(){return o},default:function(){return u},frontMatter:function(){return a},metadata:function(){return t},toc:function(){return c}});var t=JSON.parse('{"id":"tools/prometheus","title":"Prometheus","description":"Overview","source":"@site/docs/tools/prometheus.md","sourceDirName":"tools","slug":"/tools/prometheus","permalink":"/docs/tools/prometheus","draft":false,"unlisted":false,"editUrl":"https://github.com/MLops-Project-Platform/documentation/tree/main/docs/tools/prometheus.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"InfluxDB","permalink":"/docs/tools/influxdb"},"next":{"title":"Message Queues and Event Management","permalink":"/docs/tools/message-queues"}}'),i=n(4848),s=n(8453);const a={sidebar_position:2},o="Prometheus",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Key Features",id:"key-features",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Installation",id:"installation",level:2},{value:"Docker Compose",id:"docker-compose",level:3},{value:"Configuration",id:"configuration",level:2},{value:"prometheus.yml",id:"prometheusyml",level:3},{value:"Metrics Collection",id:"metrics-collection",level:2},{value:"Python Application Metrics",id:"python-application-metrics",level:3},{value:"GPU Metrics Exporter",id:"gpu-metrics-exporter",level:3},{value:"Querying with PromQL",id:"querying-with-promql",level:2},{value:"Basic Queries",id:"basic-queries",level:3},{value:"Aggregations",id:"aggregations",level:3},{value:"Advanced Queries",id:"advanced-queries",level:3},{value:"Alerting",id:"alerting",level:2},{value:"Alert Rules",id:"alert-rules",level:3},{value:"AlertManager Configuration",id:"alertmanager-configuration",level:3},{value:"Recording Rules",id:"recording-rules",level:2},{value:"Pre-computed Aggregations",id:"pre-computed-aggregations",level:3},{value:"Kubernetes Integration",id:"kubernetes-integration",level:2},{value:"Prometheus on Kubernetes",id:"prometheus-on-kubernetes",level:3},{value:"Monitoring Training Jobs",id:"monitoring-training-jobs",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Further Reading",id:"further-reading",level:2}];function m(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"prometheus",children:"Prometheus"})}),"\n",(0,i.jsx)(r.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(r.p,{children:"Prometheus is an open-source monitoring and alerting system with a powerful time-series database. It's perfect for collecting metrics from Kubernetes clusters and ML infrastructure."}),"\n",(0,i.jsx)(r.h2,{id:"key-features",children:"Key Features"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Pull-Based"})," - Scrapes metrics from targets"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"PromQL"})," - Powerful query language for metrics"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Time-Series Database"})," - Efficient metric storage"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Alerting"})," - Built-in alertmanager integration"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Service Discovery"})," - Auto-discovery of targets"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Multi-dimensional"})," - Labels for organization"]}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"architecture",children:"Architecture"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{children:"Exporters (Node, GPU, Custom)\r\n        \u2193\r\n    Prometheus (scrape + store)\r\n        \u2193\r\n    Alertmanager (routing)\r\n        \u2193\r\n    Notification channels (Slack, PagerDuty, etc.)\n"})}),"\n",(0,i.jsx)(r.h2,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(r.h3,{id:"docker-compose",children:"Docker Compose"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-yaml",children:'version: \'3.8\'\r\nservices:\r\n  prometheus:\r\n    image: prom/prometheus:latest\r\n    ports:\r\n      - "9090:9090"\r\n    volumes:\r\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\r\n      - prometheus_data:/prometheus\r\n    command:\r\n      - \'--config.file=/etc/prometheus/prometheus.yml\'\r\n      - \'--storage.tsdb.path=/prometheus\'\r\n      - \'--storage.tsdb.retention.time=30d\'\r\n    healthcheck:\r\n      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]\r\n      interval: 30s\r\n      timeout: 10s\r\n      retries: 5\r\n\r\n  alertmanager:\r\n    image: prom/alertmanager:latest\r\n    ports:\r\n      - "9093:9093"\r\n    volumes:\r\n      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml\r\n      - alertmanager_data:/alertmanager\r\n\r\nvolumes:\r\n  prometheus_data:\r\n  alertmanager_data:\n'})}),"\n",(0,i.jsx)(r.h2,{id:"configuration",children:"Configuration"}),"\n",(0,i.jsx)(r.h3,{id:"prometheusyml",children:"prometheus.yml"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-yaml",children:"global:\r\n  scrape_interval: 15s\r\n  evaluation_interval: 15s\r\n  external_labels:\r\n    cluster: 'mlops-prod'\r\n    environment: 'production'\r\n\r\nalerting:\r\n  alertmanagers:\r\n    - static_configs:\r\n        - targets:\r\n            - 'alertmanager:9093'\r\n\r\nrule_files:\r\n  - 'alert_rules.yml'\r\n  - 'recording_rules.yml'\r\n\r\nscrape_configs:\r\n  # Prometheus itself\r\n  - job_name: 'prometheus'\r\n    static_configs:\r\n      - targets: ['localhost:9090']\r\n\r\n  # Node exporter (server metrics)\r\n  - job_name: 'node'\r\n    static_configs:\r\n      - targets: ['node-exporter:9100']\r\n\r\n  # NVIDIA GPU metrics\r\n  - job_name: 'gpu'\r\n    static_configs:\r\n      - targets: ['gpu-exporter:9445']\r\n\r\n  # Custom training metrics\r\n  - job_name: 'training'\r\n    scrape_interval: 5s\r\n    static_configs:\r\n      - targets: ['training-app:8000']\r\n\r\n  # Kubernetes service discovery\r\n  - job_name: 'kubernetes-pods'\r\n    kubernetes_sd_configs:\r\n      - role: pod\r\n    relabel_configs:\r\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\r\n        action: keep\r\n        regex: true\r\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\r\n        action: replace\r\n        target_label: __metrics_path__\r\n        regex: (.+)\r\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\r\n        action: replace\r\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\r\n        replacement: $1:$2\r\n        target_label: __address__\n"})}),"\n",(0,i.jsx)(r.h2,{id:"metrics-collection",children:"Metrics Collection"}),"\n",(0,i.jsx)(r.h3,{id:"python-application-metrics",children:"Python Application Metrics"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"from prometheus_client import Counter, Histogram, Gauge, start_http_server\r\nimport time\r\n\r\n# Counter: monotonically increasing\r\ntraining_steps = Counter(\r\n    'training_steps_total',\r\n    'Total training steps',\r\n    ['experiment_id', 'model_name']\r\n)\r\n\r\n# Gauge: can go up or down\r\ngpu_memory_usage = Gauge(\r\n    'gpu_memory_mb',\r\n    'GPU memory usage in MB',\r\n    ['gpu_id', 'experiment_id']\r\n)\r\n\r\n# Histogram: observe value distributions\r\ntraining_loss = Histogram(\r\n    'training_loss',\r\n    'Training loss distribution',\r\n    ['experiment_id'],\r\n    buckets=(0.001, 0.01, 0.1, 0.5, 1.0, 2.0)\r\n)\r\n\r\n# Start metrics server on port 8000\r\nstart_http_server(8000)\r\n\r\n# Record metrics during training\r\nfor epoch in range(100):\r\n    loss = 0.5 / (epoch + 1)\r\n    \r\n    training_steps.labels(experiment_id='exp_001', model_name='bert').inc()\r\n    training_loss.labels(experiment_id='exp_001').observe(loss)\r\n    gpu_memory_usage.labels(gpu_id='0', experiment_id='exp_001').set(4096)\r\n    \r\n    time.sleep(1)\n"})}),"\n",(0,i.jsx)(r.h3,{id:"gpu-metrics-exporter",children:"GPU Metrics Exporter"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"import subprocess\r\nimport time\r\nfrom prometheus_client import Gauge, start_http_server\r\n\r\n# GPU memory gauge\r\ngpu_memory = Gauge('nvidia_gpu_memory_used_mb', 'GPU memory used', ['gpu_id'])\r\ngpu_utilization = Gauge('nvidia_gpu_utilization_percent', 'GPU utilization', ['gpu_id'])\r\n\r\ndef collect_gpu_metrics():\r\n    while True:\r\n        try:\r\n            # nvidia-smi query\r\n            result = subprocess.run([\r\n                'nvidia-smi',\r\n                '--query-gpu=index,memory.used,utilization.gpu',\r\n                '--format=csv,noheader'\r\n            ], capture_output=True, text=True)\r\n            \r\n            for line in result.stdout.strip().split('\\n'):\r\n                gpu_id, memory, utilization = line.split(',')\r\n                gpu_memory.labels(gpu_id=gpu_id.strip()).set(memory.strip().split()[0])\r\n                gpu_utilization.labels(gpu_id=gpu_id.strip()).set(utilization.strip().split()[0])\r\n        \r\n        except Exception as e:\r\n            print(f\"Error collecting GPU metrics: {e}\")\r\n        \r\n        time.sleep(15)\r\n\r\nstart_http_server(9445)\r\n\r\nimport threading\r\nthread = threading.Thread(target=collect_gpu_metrics, daemon=True)\r\nthread.start()\r\n\r\n# Keep running\r\nwhile True:\r\n    time.sleep(1)\n"})}),"\n",(0,i.jsx)(r.h2,{id:"querying-with-promql",children:"Querying with PromQL"}),"\n",(0,i.jsx)(r.h3,{id:"basic-queries",children:"Basic Queries"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-promql",children:'# Last value\r\ntraining_loss{experiment_id="exp_001"}\r\n\r\n# Range (last hour)\r\ntraining_loss{experiment_id="exp_001"}[1h]\r\n\r\n# Rate (per second over 5 minutes)\r\nrate(training_steps_total[5m])\r\n\r\n# Increase (total increase over period)\r\nincrease(training_steps_total[1h])\n'})}),"\n",(0,i.jsx)(r.h3,{id:"aggregations",children:"Aggregations"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-promql",children:"# Sum across all GPU IDs\r\nsum(gpu_memory_mb) by (experiment_id)\r\n\r\n# Average GPU utilization\r\navg(nvidia_gpu_utilization_percent) by (gpu_id)\r\n\r\n# Percentiles\r\nhistogram_quantile(0.95, training_loss)\r\n\r\n# Count of distinct experiments\r\ncount(count by (experiment_id)(training_steps_total))\n"})}),"\n",(0,i.jsx)(r.h3,{id:"advanced-queries",children:"Advanced Queries"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-promql",children:'# Loss derivative (rate of change)\r\nderiv(training_loss{experiment_id="exp_001"}[5m])\r\n\r\n# Top 5 experiments by total steps\r\ntopk(5, sum by (experiment_id) (training_steps_total))\r\n\r\n# GPU memory growth rate\r\nrate(gpu_memory_mb{gpu_id="0"}[1h])\r\n\r\n# Experiments with slow loss decrease\r\nrate(training_loss{experiment_id=~"exp_.*"}[10m]) > 0.001\n'})}),"\n",(0,i.jsx)(r.h2,{id:"alerting",children:"Alerting"}),"\n",(0,i.jsx)(r.h3,{id:"alert-rules",children:"Alert Rules"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-yaml",children:'# alert_rules.yml\r\ngroups:\r\n  - name: training_alerts\r\n    interval: 30s\r\n    rules:\r\n      # Alert on high loss\r\n      - alert: HighTrainingLoss\r\n        expr: training_loss > 1.0\r\n        for: 5m\r\n        annotations:\r\n          summary: "High training loss for {{ $labels.experiment_id }}"\r\n          description: "Loss is {{ $value }}"\r\n\r\n      # Alert on GPU memory high\r\n      - alert: HighGPUMemory\r\n        expr: gpu_memory_mb > 15000\r\n        for: 2m\r\n        annotations:\r\n          summary: "High GPU memory on {{ $labels.gpu_id }}"\r\n          description: "GPU memory: {{ $value }}MB"\r\n\r\n      # Alert on training stalled\r\n      - alert: TrainingStalled\r\n        expr: rate(training_steps_total[5m]) == 0\r\n        for: 10m\r\n        annotations:\r\n          summary: "Training stalled for {{ $labels.experiment_id }}"\r\n          description: "No training steps in last 5 minutes"\r\n\r\n      # Alert on low accuracy\r\n      - alert: LowModelAccuracy\r\n        expr: training_accuracy < 0.5\r\n        for: 30m\r\n        annotations:\r\n          summary: "Low accuracy for {{ $labels.experiment_id }}"\r\n          description: "Accuracy: {{ $value }}"\n'})}),"\n",(0,i.jsx)(r.h3,{id:"alertmanager-configuration",children:"AlertManager Configuration"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-yaml",children:"# alertmanager.yml\r\nglobal:\r\n  resolve_timeout: 5m\r\n\r\nroute:\r\n  receiver: 'default'\r\n  group_by: ['alertname', 'experiment_id']\r\n  group_wait: 10s\r\n  group_interval: 10s\r\n  repeat_interval: 12h\r\n\r\n  # Routing rules\r\n  routes:\r\n    - match:\r\n        alertname: TrainingStalled\r\n      receiver: 'critical'\r\n      group_wait: 5s\r\n      repeat_interval: 30m\r\n\r\n    - match:\r\n        severity: warning\r\n      receiver: 'warnings'\r\n\r\nreceivers:\r\n  - name: 'default'\r\n    slack_configs:\r\n      - api_url: 'YOUR_SLACK_WEBHOOK'\r\n        channel: '#mlops-alerts'\r\n\r\n  - name: 'critical'\r\n    slack_configs:\r\n      - api_url: 'YOUR_SLACK_WEBHOOK'\r\n        channel: '#mlops-critical'\r\n    pagerduty_configs:\r\n      - service_key: 'YOUR_PAGERDUTY_KEY'\r\n\r\n  - name: 'warnings'\r\n    email_configs:\r\n      - to: 'team@example.com'\r\n        from: 'alerts@example.com'\r\n        smarthost: 'smtp.example.com:587'\n"})}),"\n",(0,i.jsx)(r.h2,{id:"recording-rules",children:"Recording Rules"}),"\n",(0,i.jsx)(r.h3,{id:"pre-computed-aggregations",children:"Pre-computed Aggregations"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-yaml",children:"# recording_rules.yml\r\ngroups:\r\n  - name: training_metrics\r\n    interval: 1m\r\n    rules:\r\n      # Pre-compute average loss per experiment\r\n      - record: training:loss:avg\r\n        expr: avg(training_loss) by (experiment_id)\r\n\r\n      # Pre-compute training throughput\r\n      - record: training:throughput:rate5m\r\n        expr: rate(training_steps_total[5m])\r\n\r\n      # Pre-compute GPU usage average per node\r\n      - record: gpu:memory:avg:node\r\n        expr: avg(gpu_memory_mb) by (node_id)\r\n\r\n      # Pre-compute accuracy trend\r\n      - record: training:accuracy:1h_avg\r\n        expr: avg_over_time(training_accuracy[1h])\n"})}),"\n",(0,i.jsx)(r.h2,{id:"kubernetes-integration",children:"Kubernetes Integration"}),"\n",(0,i.jsx)(r.h3,{id:"prometheus-on-kubernetes",children:"Prometheus on Kubernetes"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-yaml",children:"apiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: prometheus-config\r\ndata:\r\n  prometheus.yml: |\r\n    global:\r\n      scrape_interval: 15s\r\n    scrape_configs:\r\n      - job_name: 'kubernetes-apiservers'\r\n        kubernetes_sd_configs:\r\n          - role: endpoints\r\n        scheme: https\r\n        tls_config:\r\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\r\n\r\n---\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: prometheus\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: prometheus\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: prometheus\r\n    spec:\r\n      serviceAccountName: prometheus\r\n      containers:\r\n      - name: prometheus\r\n        image: prom/prometheus:latest\r\n        ports:\r\n        - containerPort: 9090\r\n        volumeMounts:\r\n        - name: config\r\n          mountPath: /etc/prometheus\r\n        - name: storage\r\n          mountPath: /prometheus\r\n        args:\r\n          - '--config.file=/etc/prometheus/prometheus.yml'\r\n          - '--storage.tsdb.path=/prometheus'\r\n      volumes:\r\n      - name: config\r\n        configMap:\r\n          name: prometheus-config\r\n      - name: storage\r\n        emptyDir: {}\n"})}),"\n",(0,i.jsx)(r.h2,{id:"monitoring-training-jobs",children:"Monitoring Training Jobs"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"from prometheus_client import Counter, Histogram, Gauge, start_http_server\r\n\r\n# Custom metrics for ML training\r\nmodel_training_epochs = Counter(\r\n    'model_training_epochs_total',\r\n    'Total epochs completed',\r\n    ['experiment_id', 'model_name', 'dataset']\r\n)\r\n\r\ntraining_duration_seconds = Histogram(\r\n    'training_duration_seconds',\r\n    'Training duration',\r\n    ['experiment_id'],\r\n    buckets=(60, 300, 900, 1800, 3600, 7200)\r\n)\r\n\r\nmodel_best_accuracy = Gauge(\r\n    'model_best_accuracy',\r\n    'Best validation accuracy',\r\n    ['experiment_id', 'model_name']\r\n)\r\n\r\nbatch_processing_time = Histogram(\r\n    'batch_processing_ms',\r\n    'Batch processing time in milliseconds',\r\n    ['experiment_id', 'batch_size'],\r\n    buckets=(10, 50, 100, 500, 1000)\r\n)\r\n\r\n# Use in training\r\nfor epoch in range(num_epochs):\r\n    for batch in dataloader:\r\n        start = time.time()\r\n        \r\n        # Training\r\n        output = model(batch)\r\n        loss = criterion(output, batch.labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        # Record metrics\r\n        batch_processing_time.labels(\r\n            experiment_id=exp_id,\r\n            batch_size=len(batch)\r\n        ).observe((time.time() - start) * 1000)\r\n    \r\n    model_training_epochs.labels(\r\n        experiment_id=exp_id,\r\n        model_name='bert',\r\n        dataset='dataset_v1'\r\n    ).inc()\r\n    \r\n    # Validation\r\n    accuracy = validate()\r\n    model_best_accuracy.labels(\r\n        experiment_id=exp_id,\r\n        model_name='bert'\r\n    ).set(accuracy)\n"})}),"\n",(0,i.jsx)(r.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Use appropriate labels"})," - Keep cardinality reasonable"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Set retention"})," - 30 days typical for live metrics"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Use recording rules"})," - Pre-compute expensive queries"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Alert on symptoms"})," - Not causes (e.g., high temperature > hardware failure)"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Monitor cardinality"})," - Prevent metric explosion"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Use relabeling"})," - Filter unnecessary targets"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Document dashboards"})," - Explain metrics"]}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://prometheus.io/docs/",children:"Prometheus Documentation"})}),"\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://prometheus.io/docs/prometheus/latest/querying/operators/",children:"PromQL Operators"})}),"\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://prometheus.io/docs/practices/naming/",children:"Best Practices"})}),"\n",(0,i.jsx)(r.li,{children:(0,i.jsx)(r.a,{href:"https://prometheus.io/docs/instrumenting/clientlibs/",children:"Client Libraries"})}),"\n"]})]})}function u(e={}){const{wrapper:r}={...(0,s.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8453:function(e,r,n){n.d(r,{R:function(){return a},x:function(){return o}});var t=n(6540);const i={},s=t.createContext(i);function a(e){const r=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:r},e.children)}}}]);