"use strict";(self.webpackChunkmlops_documentation=self.webpackChunkmlops_documentation||[]).push([[9089],{6184:function(n,r,e){e.r(r),e.d(r,{assets:function(){return o},contentTitle:function(){return s},default:function(){return m},frontMatter:function(){return l},metadata:function(){return a},toc:function(){return c}});var a=JSON.parse('{"id":"researcher-guide/training","title":"Training Guide","description":"Learn how to write, organize, and execute training code that integrates with the MLOps Platform.","source":"@site/docs/researcher-guide/training.md","sourceDirName":"researcher-guide","slug":"/researcher-guide/training","permalink":"/docs/researcher-guide/training","draft":false,"unlisted":false,"editUrl":"https://github.com/MLops-Project-Platform/documentation/tree/main/docs/researcher-guide/training.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Setup Guide","permalink":"/docs/researcher-guide/setup"},"next":{"title":"Tracking Experiments","permalink":"/docs/researcher-guide/tracking-experiments"}}'),i=e(4848),t=e(8453);const l={sidebar_position:3},s="Training Guide",o={},c=[{value:"Training Script Structure",id:"training-script-structure",level:2},{value:"Basic Template",id:"basic-template",level:3},{value:"Configuration File",id:"configuration-file",level:3},{value:"Running Training",id:"running-training",level:2},{value:"Basic Execution",id:"basic-execution",level:3},{value:"Using Different Configs",id:"using-different-configs",level:3},{value:"Logging Metrics",id:"logging-metrics",level:2},{value:"Simple Metrics",id:"simple-metrics",level:3},{value:"Multiple Metrics at Once",id:"multiple-metrics-at-once",level:3},{value:"Metrics Over Time",id:"metrics-over-time",level:3},{value:"Logging Parameters",id:"logging-parameters",level:2},{value:"Single Parameter",id:"single-parameter",level:3},{value:"Multiple Parameters",id:"multiple-parameters",level:3},{value:"Parameters from Config",id:"parameters-from-config",level:3},{value:"Logging Artifacts",id:"logging-artifacts",level:2},{value:"Save and Log Models",id:"save-and-log-models",level:3},{value:"Log Plots and Visualizations",id:"log-plots-and-visualizations",level:3},{value:"Log Directories",id:"log-directories",level:3},{value:"Log Text and Tables",id:"log-text-and-tables",level:3},{value:"Dataset Handling",id:"dataset-handling",level:2},{value:"Load Data from Local Files",id:"load-data-from-local-files",level:3},{value:"Use Built-in Datasets",id:"use-built-in-datasets",level:3},{value:"Download External Data",id:"download-external-data",level:3},{value:"Advanced Patterns",id:"advanced-patterns",level:2},{value:"Custom Metrics",id:"custom-metrics",level:3},{value:"Nested Runs",id:"nested-runs",level:3},{value:"Tagging Runs",id:"tagging-runs",level:3},{value:"Error Handling",id:"error-handling",level:2},{value:"Try-Catch Pattern",id:"try-catch-pattern",level:3},{value:"Validation",id:"validation",level:3},{value:"Jupyter Notebook Training",id:"jupyter-notebook-training",level:2},{value:"In Notebooks",id:"in-notebooks",level:3},{value:"Share Notebooks",id:"share-notebooks",level:3},{value:"Best Practices",id:"best-practices",level:2}];function d(n){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"training-guide",children:"Training Guide"})}),"\n",(0,i.jsx)(r.p,{children:"Learn how to write, organize, and execute training code that integrates with the MLOps Platform."}),"\n",(0,i.jsx)(r.h2,{id:"training-script-structure",children:"Training Script Structure"}),"\n",(0,i.jsx)(r.h3,{id:"basic-template",children:"Basic Template"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"# src/train.py\r\nimport yaml\r\nimport argparse\r\nimport mlflow\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.metrics import accuracy_score\r\n\r\ndef load_config(config_path):\r\n    \"\"\"Load YAML configuration file.\"\"\"\r\n    with open(config_path, 'r') as f:\r\n        return yaml.safe_load(f)\r\n\r\ndef train(config):\r\n    \"\"\"Main training function.\"\"\"\r\n    # Load data\r\n    X, y = load_iris(return_X_y=True)\r\n    \r\n    # Split data\r\n    X_train, X_test, y_train, y_test = train_test_split(\r\n        X, y,\r\n        test_size=config['training']['test_size'],\r\n        random_state=config['training']['random_state']\r\n    )\r\n    \r\n    # Set MLflow experiment\r\n    mlflow.set_tracking_uri(config['mlflow']['tracking_uri'])\r\n    mlflow.set_experiment(config['mlflow']['experiment_name'])\r\n    \r\n    with mlflow.start_run(run_name=config['mlflow']['run_name']):\r\n        # Log hyperparameters\r\n        for param_name, param_value in config['model']['params'].items():\r\n            mlflow.log_param(param_name, param_value)\r\n        \r\n        # Train model\r\n        model = RandomForestClassifier(**config['model']['params'])\r\n        model.fit(X_train, y_train)\r\n        \r\n        # Evaluate\r\n        y_pred = model.predict(X_test)\r\n        accuracy = accuracy_score(y_test, y_pred)\r\n        \r\n        # Log metrics\r\n        mlflow.log_metric(\"accuracy\", accuracy)\r\n        \r\n        # Log model\r\n        mlflow.sklearn.log_model(model, \"model\")\r\n        \r\n        return accuracy\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--config', default='configs/default.yaml')\r\n    args = parser.parse_args()\r\n    \r\n    config = load_config(args.config)\r\n    accuracy = train(config)\r\n    print(f\"Model accuracy: {accuracy:.4f}\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\n"})}),"\n",(0,i.jsx)(r.h3,{id:"configuration-file",children:"Configuration File"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-yaml",children:'# configs/default.yaml\r\nmodel:\r\n  name: "random_forest"\r\n  params:\r\n    n_estimators: 100\r\n    max_depth: 10\r\n    min_samples_split: 2\r\n    min_samples_leaf: 1\r\n    random_state: 42\r\n\r\ntraining:\r\n  test_size: 0.2\r\n  validation_split: 0.1\r\n  random_state: 42\r\n\r\nmlflow:\r\n  tracking_uri: "http://localhost:5000"\r\n  experiment_name: "iris_classification"\r\n  run_name: "baseline"\n'})}),"\n",(0,i.jsx)(r.h2,{id:"running-training",children:"Running Training"}),"\n",(0,i.jsx)(r.h3,{id:"basic-execution",children:"Basic Execution"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:'# With default config\r\npython src/train.py\r\n\r\n# With custom config\r\npython src/train.py --config configs/custom.yaml\r\n\r\n# From notebook\r\nimport subprocess\r\nsubprocess.run(["python", "src/train.py"])\n'})}),"\n",(0,i.jsx)(r.h3,{id:"using-different-configs",children:"Using Different Configs"}),"\n",(0,i.jsx)(r.p,{children:"Create experiment-specific configs:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-yaml",children:'# configs/experiment_v1.yaml\r\nmodel:\r\n  name: "random_forest"\r\n  params:\r\n    n_estimators: 50\r\n    max_depth: 5\r\n\r\n# configs/experiment_v2.yaml\r\nmodel:\r\n  name: "random_forest"\r\n  params:\r\n    n_estimators: 200\r\n    max_depth: 20\n'})}),"\n",(0,i.jsx)(r.p,{children:"Run experiments:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"python src/train.py --config configs/experiment_v1.yaml\r\npython src/train.py --config configs/experiment_v2.yaml\n"})}),"\n",(0,i.jsx)(r.h2,{id:"logging-metrics",children:"Logging Metrics"}),"\n",(0,i.jsx)(r.h3,{id:"simple-metrics",children:"Simple Metrics"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\n\r\nwith mlflow.start_run():\r\n    # Single metric\r\n    mlflow.log_metric("accuracy", 0.95)\r\n    \r\n    # Multiple calls (different epochs)\r\n    for epoch in range(10):\r\n        loss = train_epoch()\r\n        mlflow.log_metric("loss", loss, step=epoch)\n'})}),"\n",(0,i.jsx)(r.h3,{id:"multiple-metrics-at-once",children:"Multiple Metrics at Once"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'metrics = {\r\n    "accuracy": 0.95,\r\n    "precision": 0.93,\r\n    "recall": 0.94,\r\n    "f1": 0.935\r\n}\r\nmlflow.log_metrics(metrics)\n'})}),"\n",(0,i.jsx)(r.h3,{id:"metrics-over-time",children:"Metrics Over Time"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\n\r\nwith mlflow.start_run():\r\n    for epoch in range(100):\r\n        train_loss = train_epoch()\r\n        val_loss = validate()\r\n        \r\n        # Log with step (for graphs)\r\n        mlflow.log_metric("train_loss", train_loss, step=epoch)\r\n        mlflow.log_metric("val_loss", val_loss, step=epoch)\n'})}),"\n",(0,i.jsx)(r.h2,{id:"logging-parameters",children:"Logging Parameters"}),"\n",(0,i.jsx)(r.h3,{id:"single-parameter",children:"Single Parameter"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'mlflow.log_param("learning_rate", 0.001)\n'})}),"\n",(0,i.jsx)(r.h3,{id:"multiple-parameters",children:"Multiple Parameters"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Dictionary of parameters\r\nhyperparams = {\r\n    "learning_rate": 0.001,\r\n    "batch_size": 32,\r\n    "epochs": 100,\r\n    "optimizer": "adam"\r\n}\r\nmlflow.log_params(hyperparams)\n'})}),"\n",(0,i.jsx)(r.h3,{id:"parameters-from-config",children:"Parameters from Config"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'def log_config_params(config):\r\n    """Log all model parameters from config."""\r\n    model_params = config.get(\'model\', {}).get(\'params\', {})\r\n    for param_name, param_value in model_params.items():\r\n        mlflow.log_param(param_name, param_value)\r\n\r\n# In training\r\nlog_config_params(config)\n'})}),"\n",(0,i.jsx)(r.h2,{id:"logging-artifacts",children:"Logging Artifacts"}),"\n",(0,i.jsx)(r.h3,{id:"save-and-log-models",children:"Save and Log Models"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"import pickle\r\nimport mlflow\r\n\r\nwith mlflow.start_run():\r\n    # Train model...\r\n    \r\n    # Save to file\r\n    with open('model.pkl', 'wb') as f:\r\n        pickle.dump(model, f)\r\n    \r\n    # Log to MLflow\r\n    mlflow.log_artifact('model.pkl')\r\n    \r\n    # Or use MLflow serialization\r\n    mlflow.sklearn.log_model(model, \"model\")\n"})}),"\n",(0,i.jsx)(r.h3,{id:"log-plots-and-visualizations",children:"Log Plots and Visualizations"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"import matplotlib.pyplot as plt\r\nimport mlflow\r\n\r\nwith mlflow.start_run():\r\n    # Create plot\r\n    plt.figure(figsize=(10, 6))\r\n    plt.plot(train_losses, label='Train')\r\n    plt.plot(val_losses, label='Validation')\r\n    plt.xlabel('Epoch')\r\n    plt.ylabel('Loss')\r\n    plt.legend()\r\n    \r\n    # Save\r\n    plt.savefig('loss_curve.png', dpi=100, bbox_inches='tight')\r\n    plt.close()\r\n    \r\n    # Log to MLflow\r\n    mlflow.log_artifact('loss_curve.png')\n"})}),"\n",(0,i.jsx)(r.h3,{id:"log-directories",children:"Log Directories"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"import mlflow\r\n\r\nwith mlflow.start_run():\r\n    # Log all files in directory\r\n    mlflow.log_artifacts('reports/')  # Logs all files in reports/\r\n    \r\n    # Organize with subdirectories\r\n    mlflow.log_artifacts('plots/', artifact_path='visualizations')\n"})}),"\n",(0,i.jsx)(r.h3,{id:"log-text-and-tables",children:"Log Text and Tables"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"import mlflow\r\nimport pandas as pd\r\n\r\nwith mlflow.start_run():\r\n    # Log text file\r\n    with open('summary.txt', 'w') as f:\r\n        f.write(\"Model Summary\\n\")\r\n        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\r\n    mlflow.log_artifact('summary.txt')\r\n    \r\n    # Log pandas DataFrame as artifact\r\n    results_df = pd.DataFrame({\r\n        'metric': ['accuracy', 'precision', 'recall'],\r\n        'value': [0.95, 0.93, 0.94]\r\n    })\r\n    results_df.to_csv('results.csv', index=False)\r\n    mlflow.log_artifact('results.csv')\n"})}),"\n",(0,i.jsx)(r.h2,{id:"dataset-handling",children:"Dataset Handling"}),"\n",(0,i.jsx)(r.h3,{id:"load-data-from-local-files",children:"Load Data from Local Files"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"import pandas as pd\r\n\r\ndef load_data(data_path):\r\n    \"\"\"Load CSV data.\"\"\"\r\n    df = pd.read_csv(data_path)\r\n    return df\r\n\r\n# In training\r\ndata = load_data('data/train.csv')\r\nX = data.drop('target', axis=1)\r\ny = data['target']\n"})}),"\n",(0,i.jsx)(r.h3,{id:"use-built-in-datasets",children:"Use Built-in Datasets"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"from sklearn.datasets import load_iris, load_wine\r\nimport pandas as pd\r\n\r\n# Load built-in dataset\r\nX, y = load_iris(return_X_y=True)\r\n\r\n# Convert to DataFrame for easier handling\r\nimport numpy as np\r\nfeature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\r\ndf = pd.DataFrame(X, columns=feature_names)\r\ndf['target'] = y\n"})}),"\n",(0,i.jsx)(r.h3,{id:"download-external-data",children:"Download External Data"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import urllib.request\r\n\r\ndef download_data(url, filename):\r\n    """Download data from URL."""\r\n    print(f"Downloading {url}...")\r\n    urllib.request.urlretrieve(url, filename)\r\n    return filename\r\n\r\n# In training\r\ndata_file = download_data(\r\n    \'https://example.com/data.csv\',\r\n    \'data/external_data.csv\'\r\n)\r\ndf = pd.read_csv(data_file)\n'})}),"\n",(0,i.jsx)(r.h2,{id:"advanced-patterns",children:"Advanced Patterns"}),"\n",(0,i.jsx)(r.h3,{id:"custom-metrics",children:"Custom Metrics"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\nfrom sklearn.metrics import precision_recall_curve\r\nimport numpy as np\r\n\r\nwith mlflow.start_run():\r\n    # Log custom metrics\r\n    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\r\n    \r\n    mlflow.log_metric("max_precision", np.max(precision))\r\n    mlflow.log_metric("max_recall", np.max(recall))\n'})}),"\n",(0,i.jsx)(r.h3,{id:"nested-runs",children:"Nested Runs"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\n\r\nwith mlflow.start_run(run_name="parent_run"):\r\n    mlflow.log_param("version", "v1")\r\n    \r\n    # Child run 1\r\n    with mlflow.start_run(nested=True, run_name="experiment_1"):\r\n        mlflow.log_metric("accuracy", 0.90)\r\n    \r\n    # Child run 2\r\n    with mlflow.start_run(nested=True, run_name="experiment_2"):\r\n        mlflow.log_metric("accuracy", 0.95)\n'})}),"\n",(0,i.jsx)(r.h3,{id:"tagging-runs",children:"Tagging Runs"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\n\r\nwith mlflow.start_run():\r\n    # Add tags for organization\r\n    mlflow.set_tag("data_version", "v2.0")\r\n    mlflow.set_tag("author", "alice")\r\n    mlflow.set_tag("status", "completed")\r\n    \r\n    # Multiple tags\r\n    mlflow.set_tags({\r\n        "team": "ml-research",\r\n        "project": "iris-classification",\r\n        "branch": "main"\r\n    })\n'})}),"\n",(0,i.jsx)(r.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,i.jsx)(r.h3,{id:"try-catch-pattern",children:"Try-Catch Pattern"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\n\r\nwith mlflow.start_run():\r\n    try:\r\n        # Training code\r\n        model = train()\r\n        metrics = evaluate(model)\r\n        mlflow.log_metrics(metrics)\r\n        \r\n    except Exception as e:\r\n        mlflow.set_tag("status", "failed")\r\n        mlflow.set_tag("error", str(e))\r\n        print(f"Training failed: {e}")\r\n        raise\r\n    \r\n    finally:\r\n        # Cleanup if needed\r\n        mlflow.set_tag("finished", True)\n'})}),"\n",(0,i.jsx)(r.h3,{id:"validation",children:"Validation"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\n\r\ndef validate_data(X, y):\r\n    """Validate data before training."""\r\n    assert len(X) > 0, "Data cannot be empty"\r\n    assert len(X) == len(y), "X and y must have same length"\r\n    return True\r\n\r\nwith mlflow.start_run():\r\n    validate_data(X, y)\r\n    # Proceed with training\n'})}),"\n",(0,i.jsx)(r.h2,{id:"jupyter-notebook-training",children:"Jupyter Notebook Training"}),"\n",(0,i.jsx)(r.h3,{id:"in-notebooks",children:"In Notebooks"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# In Jupyter cell\r\nimport mlflow\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.metrics import accuracy_score\r\n\r\n# Load and prepare data\r\nfrom sklearn.datasets import load_iris\r\nX, y = load_iris(return_X_y=True)\r\n\r\nmlflow.set_tracking_uri("http://localhost:5000")\r\nmlflow.set_experiment("notebook_experiments")\r\n\r\n# Train\r\nwith mlflow.start_run(run_name="notebook_run"):\r\n    model = RandomForestClassifier(n_estimators=100)\r\n    model.fit(X[:100], y[:100])\r\n    \r\n    # Evaluate\r\n    score = model.score(X[100:], y[100:])\r\n    mlflow.log_metric("accuracy", score)\r\n    \r\n    print(f"Accuracy: {score:.4f}")\r\n    print("Run logged to MLflow!")\n'})}),"\n",(0,i.jsx)(r.h3,{id:"share-notebooks",children:"Share Notebooks"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:'# Commit notebook to git\r\ngit add notebook.ipynb\r\ngit commit -m "Add training notebook"\n'})}),"\n",(0,i.jsx)(r.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(r.p,{children:["\u2705 ",(0,i.jsx)(r.strong,{children:"Do:"})]}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Use configuration files for all parameters"}),"\n",(0,i.jsx)(r.li,{children:"Log all hyperparameters"}),"\n",(0,i.jsx)(r.li,{children:"Log multiple metrics"}),"\n",(0,i.jsx)(r.li,{children:"Version your code with git"}),"\n",(0,i.jsx)(r.li,{children:"Use meaningful run names"}),"\n",(0,i.jsx)(r.li,{children:"Tag important runs"}),"\n",(0,i.jsx)(r.li,{children:"Log final model"}),"\n"]}),"\n",(0,i.jsxs)(r.p,{children:["\u274c ",(0,i.jsx)(r.strong,{children:"Don't:"})]}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"Hard-code parameters"}),"\n",(0,i.jsx)(r.li,{children:"Skip logging"}),"\n",(0,i.jsx)(r.li,{children:"Run without tracking"}),"\n",(0,i.jsx)(r.li,{children:"Use uncommitted code"}),"\n",(0,i.jsx)(r.li,{children:"Mix unrelated experiments"}),"\n",(0,i.jsx)(r.li,{children:"Lose track of good results"}),"\n"]}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsxs)(r.p,{children:["Next: ",(0,i.jsx)(r.a,{href:"/docs/researcher-guide/tracking-experiments",children:"Experiment Tracking \u2192"})]})]})}function m(n={}){const{wrapper:r}={...(0,t.R)(),...n.components};return r?(0,i.jsx)(r,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:function(n,r,e){e.d(r,{R:function(){return l},x:function(){return s}});var a=e(6540);const i={},t=a.createContext(i);function l(n){const r=a.useContext(t);return a.useMemo(function(){return"function"==typeof n?n(r):{...r,...n}},[r,n])}function s(n){let r;return r=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:l(n.components),a.createElement(t.Provider,{value:r},n.children)}}}]);