"use strict";(self.webpackChunkmlops_documentation=self.webpackChunkmlops_documentation||[]).push([[1442],{6957:function(e,n,r){r.r(n),r.d(n,{assets:function(){return o},contentTitle:function(){return a},default:function(){return u},frontMatter:function(){return s},metadata:function(){return i},toc:function(){return c}});var i=JSON.parse('{"id":"tools/influxdb","title":"InfluxDB","description":"Overview","source":"@site/docs/tools/influxdb.md","sourceDirName":"tools","slug":"/tools/influxdb","permalink":"/docs/tools/influxdb","draft":false,"unlisted":false,"editUrl":"https://github.com/MLops-Project-Platform/documentation/tree/main/docs/tools/influxdb.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Memcached","permalink":"/docs/tools/memcached"},"next":{"title":"Prometheus","permalink":"/docs/tools/prometheus"}}'),t=r(4848),l=r(8453);const s={sidebar_position:1},a="InfluxDB",o={},c=[{value:"Overview",id:"overview",level:2},{value:"Key Features",id:"key-features",level:2},{value:"Use Cases in MLOps",id:"use-cases-in-mlops",level:2},{value:"Training Metrics Collection",id:"training-metrics-collection",level:3},{value:"Installation",id:"installation",level:2},{value:"Docker",id:"docker",level:3},{value:"Writing Data",id:"writing-data",level:2},{value:"Line Protocol",id:"line-protocol",level:3},{value:"Batch Writing",id:"batch-writing",level:3},{value:"Querying Data",id:"querying-data",level:2},{value:"Flux Language",id:"flux-language",level:3},{value:"Aggregation",id:"aggregation",level:3},{value:"Data Retention",id:"data-retention",level:2},{value:"Retention Policies",id:"retention-policies",level:3},{value:"Downsampling (Continuous Aggregates)",id:"downsampling-continuous-aggregates",level:3},{value:"Time-Series Operations",id:"time-series-operations",level:2},{value:"Moving Averages",id:"moving-averages",level:3},{value:"Percentiles",id:"percentiles",level:3},{value:"Advanced Features",id:"advanced-features",level:2},{value:"Alerting",id:"alerting",level:3},{value:"Custom Functions",id:"custom-functions",level:3},{value:"Monitoring with InfluxDB",id:"monitoring-with-influxdb",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"influxdb",children:"InfluxDB"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"InfluxDB is a time-series database optimized for storing and querying time-stamped data. It's ideal for tracking training metrics, GPU usage, and system performance in MLOps."}),"\n",(0,t.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Time-Series Optimized"})," - Efficient compression of time-stamped data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High Write Throughput"})," - Handle millions of data points/second"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flexible Retention"})," - Auto-delete old data based on policies"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"InfluxQL & Flux"})," - Powerful query languages"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Built-in Aggregation"})," - Fast downsampling"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Clustering"})," - Enterprise clustering available"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"use-cases-in-mlops",children:"Use Cases in MLOps"}),"\n",(0,t.jsx)(n.h3,{id:"training-metrics-collection",children:"Training Metrics Collection"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from influxdb_client import InfluxDBClient\r\nfrom influxdb_client.client.write_api import SYNCHRONOUS\r\nfrom datetime import datetime\r\nimport time\r\n\r\n# Setup client\r\nclient = InfluxDBClient(url="http://localhost:8086", token="your_token", org="mlops")\r\nwrite_api = client.write_api(write_options=SYNCHRONOUS)\r\n\r\n# Write training metrics\r\ndef log_training_metrics(experiment_id, epoch, loss, accuracy, lr):\r\n    point = f"""training_metrics,experiment_id={experiment_id},epoch={epoch} \\\r\nloss={loss},accuracy={accuracy},learning_rate={lr} {int(time.time() * 1e9)}"""\r\n    \r\n    write_api.write(bucket="mlops", record=point)\r\n\r\n# Log during training\r\nfor epoch in range(1, 11):\r\n    loss = 0.1 * (10 - epoch) / 10\r\n    accuracy = 0.5 + epoch * 0.05\r\n    log_training_metrics(\'exp_001\', epoch, loss, accuracy, 0.001)\r\n    time.sleep(1)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,t.jsx)(n.h3,{id:"docker",children:"Docker"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'version: \'3.8\'\r\nservices:\r\n  influxdb:\r\n    image: influxdb:2.6\r\n    environment:\r\n      INFLUXDB_DB: mlops\r\n      INFLUXDB_ADMIN_USER: admin\r\n      INFLUXDB_ADMIN_PASSWORD: password\r\n      INFLUXDB_HTTP_AUTH_ENABLED: "true"\r\n    ports:\r\n      - "8086:8086"\r\n    volumes:\r\n      - influxdb_data:/var/lib/influxdb2\r\n    healthcheck:\r\n      test: ["CMD", "influx", "ping"]\r\n      interval: 30s\r\n      timeout: 10s\r\n      retries: 5\r\n\r\nvolumes:\r\n  influxdb_data:\n'})}),"\n",(0,t.jsx)(n.h2,{id:"writing-data",children:"Writing Data"}),"\n",(0,t.jsx)(n.h3,{id:"line-protocol",children:"Line Protocol"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from influxdb_client import InfluxDBClient\r\nfrom influxdb_client.client.write_api import SYNCHRONOUS\r\nimport time\r\n\r\nclient = InfluxDBClient(url="http://localhost:8086", token="your_token", org="mlops")\r\nwrite_api = client.write_api(write_options=SYNCHRONOUS)\r\n\r\n# Line protocol: measurement,tag1=val1,tag2=val2 field1=val1,field2=val2 timestamp\r\npoint = "gpu_usage,experiment_id=exp_001,gpu_id=0 usage=85.5,memory=2048 " + str(int(time.time() * 1e9))\r\n\r\nwrite_api.write(bucket="mlops", record=point)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"batch-writing",children:"Batch Writing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from influxdb_client import InfluxDBClient, Point\r\nfrom influxdb_client.client.write_api import SYNCHRONOUS, ASYNCHRONOUS\r\nimport time\r\n\r\nclient = InfluxDBClient(url="http://localhost:8086", token="your_token", org="mlops")\r\nwrite_api = client.write_api(write_options=ASYNCHRONOUS)\r\n\r\n# Create multiple points\r\npoints = []\r\nfor i in range(100):\r\n    point = Point("system_metrics") \\\r\n        .tag("experiment_id", "exp_001") \\\r\n        .tag("node", "worker-1") \\\r\n        .field("cpu_usage", 45.2 + i) \\\r\n        .field("memory_mb", 2048 + i * 10) \\\r\n        .field("disk_io", 1.5 + i * 0.01) \\\r\n        .time(int(time.time() * 1e9) + i * 1e9)\r\n    \r\n    points.append(point)\r\n\r\n# Write batch\r\nwrite_api.write(bucket="mlops", records=points)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"querying-data",children:"Querying Data"}),"\n",(0,t.jsx)(n.h3,{id:"flux-language",children:"Flux Language"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from influxdb_client import InfluxDBClient\r\n\r\nclient = InfluxDBClient(url="http://localhost:8086", token="your_token", org="mlops")\r\nquery_api = client.query_api()\r\n\r\n# Query last 24 hours\r\nquery = """\r\nfrom(bucket:"mlops")\r\n  |> range(start: -24h)\r\n  |> filter(fn: (r) => r._measurement == "training_metrics")\r\n  |> filter(fn: (r) => r.experiment_id == "exp_001")\r\n  |> filter(fn: (r) => r._field == "accuracy")\r\n"""\r\n\r\nresult = query_api.query(org="mlops", query=query)\r\n\r\nfor table in result:\r\n    for record in table.records:\r\n        print(f"{record.time}: {record.values[\'_value\']}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"aggregation",children:"Aggregation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'query = """\r\nfrom(bucket:"mlops")\r\n  |> range(start: -7d)\r\n  |> filter(fn: (r) => r._measurement == "training_metrics")\r\n  |> aggregateWindow(every: 1h, fn: mean)\r\n"""\r\n\r\n# Group by experiment\r\nquery = """\r\nfrom(bucket:"mlops")\r\n  |> range(start: -24h)\r\n  |> filter(fn: (r) => r._measurement == "training_metrics")\r\n  |> group(columns: ["experiment_id"])\r\n  |> aggregateWindow(every: 10m, fn: mean)\r\n"""\r\n\r\n# Calculate derivative (rate of change)\r\nquery = """\r\nfrom(bucket:"mlops")\r\n  |> range(start: -24h)\r\n  |> filter(fn: (r) => r._measurement == "training_metrics")\r\n  |> filter(fn: (r) => r._field == "loss")\r\n  |> derivative(unit: 1m)\r\n"""\n'})}),"\n",(0,t.jsx)(n.h2,{id:"data-retention",children:"Data Retention"}),"\n",(0,t.jsx)(n.h3,{id:"retention-policies",children:"Retention Policies"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Create bucket with 30-day retention\r\ninflux bucket create --name mlops --retention 30d\r\n\r\n# Infinite retention\r\ninflux bucket create --name archives --retention 0\n"})}),"\n",(0,t.jsx)(n.h3,{id:"downsampling-continuous-aggregates",children:"Downsampling (Continuous Aggregates)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'query = """\r\nimport "influxdata/influxdb/tasks"\r\n\r\noption task = {\r\n  name: "downsample-training-metrics",\r\n  every: 1h,\r\n  offset: 0m,\r\n}\r\n\r\nfrom(bucket: "mlops")\r\n  |> range(start: -1h, stop: now())\r\n  |> filter(fn: (r) => r._measurement == "training_metrics")\r\n  |> aggregateWindow(every: 1h, fn: mean, createEmpty: false)\r\n  |> to(bucket: "mlops_downsampled")\r\n"""\n'})}),"\n",(0,t.jsx)(n.h2,{id:"time-series-operations",children:"Time-Series Operations"}),"\n",(0,t.jsx)(n.h3,{id:"moving-averages",children:"Moving Averages"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'query = """\r\nfrom(bucket:"mlops")\r\n  |> range(start: -24h)\r\n  |> filter(fn: (r) => r._measurement == "training_metrics")\r\n  |> filter(fn: (r) => r._field == "loss")\r\n  |> movingAverage(n: 10)\r\n"""\r\n\r\n# Exponential moving average\r\nquery = """\r\nfrom(bucket:"mlops")\r\n  |> range(start: -24h)\r\n  |> filter(fn: (r) => r._measurement == "training_metrics")\r\n  |> filter(fn: (r) => r._field == "loss")\r\n  |> exponentialMovingAverage(n: 10)\r\n"""\n'})}),"\n",(0,t.jsx)(n.h3,{id:"percentiles",children:"Percentiles"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'query = """\r\nfrom(bucket:"mlops")\r\n  |> range(start: -24h)\r\n  |> filter(fn: (r) => r._measurement == "training_metrics")\r\n  |> filter(fn: (r) => r._field == "loss")\r\n  |> quantile(q: 0.95)\r\n"""\r\n\r\n# Multiple percentiles\r\nquery = """\r\nfrom(bucket:"mlops")\r\n  |> range(start: -24h)\r\n  |> filter(fn: (r) => r._measurement == "training_metrics")\r\n  |> filter(fn: (r) => r._field == "accuracy")\r\n  |> group(columns: ["experiment_id"])\r\n  |> quantile(q: 0.5, method: "interpolated")\r\n"""\n'})}),"\n",(0,t.jsx)(n.h2,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,t.jsx)(n.h3,{id:"alerting",children:"Alerting"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import requests\r\n\r\nalert_rule = {\r\n    "name": "High GPU Usage",\r\n    "description": "Alert when GPU usage exceeds 90%",\r\n    "every": "5m",\r\n    "offset": "0s",\r\n    "statusMessageTemplate": "GPU usage: {{ .values.usage }}",\r\n    "tags": ["gpu", "warning"],\r\n    "taskID": "task_id_here"\r\n}\r\n\r\n# Create alert via API\r\nheaders = {"Authorization": f"Token your_token"}\r\nresponse = requests.post(\r\n    "http://localhost:8086/api/v2/checks",\r\n    json=alert_rule,\r\n    headers=headers\r\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"custom-functions",children:"Custom Functions"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'query = """\r\nfrom(bucket: "mlops")\r\n  |> range(start: -24h)\r\n  |> filter(fn: (r) => r._measurement == "training_metrics")\r\n  |> map(fn: (r) => ({r with _value: r._value * 1.1}))\r\n"""\n'})}),"\n",(0,t.jsx)(n.h2,{id:"monitoring-with-influxdb",children:"Monitoring with InfluxDB"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from influxdb_client import InfluxDBClient, Point\r\nimport psutil\r\nimport time\r\n\r\nclient = InfluxDBClient(url="http://localhost:8086", token="your_token", org="mlops")\r\nwrite_api = client.write_api(write_options=ASYNCHRONOUS)\r\n\r\ndef collect_system_metrics():\r\n    while True:\r\n        point = Point("system_metrics") \\\r\n            .field("cpu_percent", psutil.cpu_percent(interval=1)) \\\r\n            .field("memory_percent", psutil.virtual_memory().percent) \\\r\n            .field("disk_percent", psutil.disk_usage(\'/\').percent) \\\r\n            .field("process_count", len(psutil.pids()))\r\n        \r\n        write_api.write(bucket="mlops", record=point)\r\n        time.sleep(10)\r\n\r\n# Run collection\r\nimport threading\r\nthread = threading.Thread(target=collect_system_metrics, daemon=True)\r\nthread.start()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use tags for dimensions"})," - Experiment ID, GPU ID (indexed)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use fields for metrics"})," - Accuracy, loss, memory (not indexed)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Batch writes"})," - Reduce network overhead"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Set retention policies"})," - Manage disk space"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use downsampling"})," - For long-term storage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Index strategically"})," - Common filter dimensions only"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Query efficiently"})," - Use time ranges and filters"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# InfluxDB configuration (influxdb.conf or env vars)\r\n[data]\r\n  cache-max-memory-bytes = 1073741824  # 1GB\r\n  max-series-db = 2000000\r\n\r\n[http]\r\n  max-body-size = 536870912  # 512MB for batch writes\r\n\r\n[storage]\r\n  max-index-log-file-size = 10485760  # 10MB\n"})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check database health\r\ncurl -i http://localhost:8086/health\r\n\r\n# Get buckets\r\ninflux bucket list --token your_token\r\n\r\n# Query from CLI\r\ninflux query --file query.flux\r\n\r\n# Check InfluxDB version\r\ninflux version\r\n\r\n# Backup database\r\ninfluxd backup /path/to/backup\r\n\r\n# Restore database\r\ninfluxd restore /path/to/backup\n"})}),"\n",(0,t.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.influxdata.com/influxdb/",children:"InfluxDB Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.influxdata.com/flux/latest/",children:"Flux Language Reference"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/influxdata/influxdb-client-python",children:"Python Client Library"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.influxdata.com/influxdb/latest/best-practices/",children:"Time-Series Best Practices"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:function(e,n,r){r.d(n,{R:function(){return s},x:function(){return a}});var i=r(6540);const t={},l=i.createContext(t);function s(e){const n=i.useContext(l);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(l.Provider,{value:n},e.children)}}}]);