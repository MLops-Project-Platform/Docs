"use strict";(self.webpackChunkmlops_documentation=self.webpackChunkmlops_documentation||[]).push([[425],{6589:function(n,r,e){e.r(r),e.d(r,{assets:function(){return l},contentTitle:function(){return o},default:function(){return m},frontMatter:function(){return s},metadata:function(){return t},toc:function(){return c}});var t=JSON.parse('{"id":"researcher-guide/best-practices","title":"Best Practices","description":"Professional approaches to ML research and experimentation.","source":"@site/docs/researcher-guide/best-practices.md","sourceDirName":"researcher-guide","slug":"/researcher-guide/best-practices","permalink":"/docs/researcher-guide/best-practices","draft":false,"unlisted":false,"editUrl":"https://github.com/MLops-Project-Platform/documentation/tree/main/docs/researcher-guide/best-practices.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Tracking Experiments","permalink":"/docs/researcher-guide/tracking-experiments"},"next":{"title":"MLOps Workflow Guide","permalink":"/docs/development/mlops-workflow"}}'),i=e(4848),a=e(8453);const s={sidebar_position:5},o="Best Practices",l={},c=[{value:"Code Organization",id:"code-organization",level:2},{value:"Project Structure",id:"project-structure",level:3},{value:".gitignore",id:"gitignore",level:3},{value:"Configuration Management",id:"configuration-management",level:2},{value:"Configuration File Best Practices",id:"configuration-file-best-practices",level:3},{value:"Load Configuration Safely",id:"load-configuration-safely",level:3},{value:"Experiment Design",id:"experiment-design",level:2},{value:"Clear Naming Convention",id:"clear-naming-convention",level:3},{value:"Systematic Hyperparameter Tuning",id:"systematic-hyperparameter-tuning",level:3},{value:"Reproducibility",id:"reproducibility",level:2},{value:"Set Seeds",id:"set-seeds",level:3},{value:"Version Everything",id:"version-everything",level:3},{value:"Commit Before Running",id:"commit-before-running",level:3},{value:"Collaboration",id:"collaboration",level:2},{value:"Meaningful Tagging",id:"meaningful-tagging",level:3},{value:"Documentation",id:"documentation",level:3},{value:"Share Results",id:"share-results",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Efficient Logging",id:"efficient-logging",level:3},{value:"Memory Efficient Evaluation",id:"memory-efficient-evaluation",level:3},{value:"Error Handling",id:"error-handling",level:2},{value:"Robust Training Loop",id:"robust-training-loop",level:3},{value:"Testing",id:"testing",level:2},{value:"Unit Tests",id:"unit-tests",level:3},{value:"Version Control",id:"version-control",level:2},{value:"Commit Messages",id:"commit-messages",level:3},{value:"Branch Strategy",id:"branch-strategy",level:3}];function d(n){const r={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"best-practices",children:"Best Practices"})}),"\n",(0,i.jsx)(r.p,{children:"Professional approaches to ML research and experimentation."}),"\n",(0,i.jsx)(r.h2,{id:"code-organization",children:"Code Organization"}),"\n",(0,i.jsx)(r.h3,{id:"project-structure",children:"Project Structure"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{children:"mlops-research-project/\r\n\u251c\u2500\u2500 configs/                    # All configuration files\r\n\u2502   \u251c\u2500\u2500 default.yaml           # Default settings\r\n\u2502   \u251c\u2500\u2500 experiment_v1.yaml     # Variant configurations\r\n\u2502   \u2514\u2500\u2500 experiment_v2.yaml\r\n\u2502\r\n\u251c\u2500\u2500 src/                        # Source code\r\n\u2502   \u251c\u2500\u2500 __init__.py\r\n\u2502   \u251c\u2500\u2500 train.py               # Main training script\r\n\u2502   \u251c\u2500\u2500 models.py              # Model definitions\r\n\u2502   \u251c\u2500\u2500 preprocessing.py       # Data preprocessing\r\n\u2502   \u251c\u2500\u2500 evaluation.py          # Evaluation metrics\r\n\u2502   \u2514\u2500\u2500 utils.py               # Utility functions\r\n\u2502\r\n\u251c\u2500\u2500 notebooks/                  # Jupyter notebooks\r\n\u2502   \u251c\u2500\u2500 01_data_exploration.ipynb\r\n\u2502   \u251c\u2500\u2500 02_model_experiments.ipynb\r\n\u2502   \u2514\u2500\u2500 03_results_analysis.ipynb\r\n\u2502\r\n\u251c\u2500\u2500 data/                       # Data directory (gitignored)\r\n\u2502   \u251c\u2500\u2500 raw/                   # Original data\r\n\u2502   \u251c\u2500\u2500 processed/             # Preprocessed data\r\n\u2502   \u2514\u2500\u2500 splits/                # Train/test splits\r\n\u2502\r\n\u251c\u2500\u2500 artifacts/                  # Local artifacts (gitignored)\r\n\u2502   \u251c\u2500\u2500 models/\r\n\u2502   \u251c\u2500\u2500 plots/\r\n\u2502   \u2514\u2500\u2500 logs/\r\n\u2502\r\n\u251c\u2500\u2500 tests/                      # Unit tests\r\n\u2502   \u251c\u2500\u2500 test_preprocessing.py\r\n\u2502   \u2514\u2500\u2500 test_models.py\r\n\u2502\r\n\u251c\u2500\u2500 requirements.txt            # Dependencies\r\n\u251c\u2500\u2500 .gitignore                 # Git ignore rules\r\n\u251c\u2500\u2500 README.md                  # Project documentation\r\n\u2514\u2500\u2500 setup.py                   # Package setup (optional)\n"})}),"\n",(0,i.jsx)(r.h3,{id:"gitignore",children:".gitignore"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{children:"# Data\r\ndata/\r\n*.csv\r\n*.json\r\n\r\n# Models and artifacts\r\nartifacts/\r\nmodels/\r\n*.pkl\r\n*.h5\r\n\r\n# Environment\r\n.venv/\r\nenv/\r\n*.egg-info/\r\n\r\n# Python\r\n__pycache__/\r\n*.pyc\r\n.pytest_cache/\r\n\r\n# Jupyter\r\n.ipynb_checkpoints/\r\n\r\n# IDE\r\n.vscode/\r\n.idea/\r\n*.swp\r\n\r\n# OS\r\n.DS_Store\r\n.env\n"})}),"\n",(0,i.jsx)(r.h2,{id:"configuration-management",children:"Configuration Management"}),"\n",(0,i.jsx)(r.h3,{id:"configuration-file-best-practices",children:"Configuration File Best Practices"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-yaml",children:'# configs/experiment.yaml\r\n# Clear, hierarchical configuration\r\n\r\n# Model hyperparameters\r\nmodel:\r\n  type: "random_forest"  # Easy to identify\r\n  params:\r\n    n_estimators: 100    # All params in one place\r\n    max_depth: 10\r\n    min_samples_split: 2\r\n    random_state: 42\r\n  \r\n# Data handling\r\ndata:\r\n  train_size: 0.8\r\n  test_size: 0.2\r\n  random_state: 42\r\n  preprocessing:\r\n    normalize: true\r\n    remove_outliers: true\r\n\r\n# Training configuration\r\ntraining:\r\n  epochs: 100\r\n  batch_size: 32\r\n  validation_split: 0.2\r\n  early_stopping:\r\n    enabled: true\r\n    patience: 10\r\n\r\n# MLflow tracking\r\nmlflow:\r\n  tracking_uri: "http://localhost:5000"\r\n  experiment_name: "baseline_experiments"\r\n  run_name: "rf_100trees_maxdepth10"\r\n  tags:\r\n    - "baseline"\r\n    - "production_candidate"\n'})}),"\n",(0,i.jsx)(r.h3,{id:"load-configuration-safely",children:"Load Configuration Safely"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import yaml\r\nfrom typing import Dict, Any\r\n\r\ndef load_config(config_path: str) -> Dict[str, Any]:\r\n    """Load and validate configuration."""\r\n    try:\r\n        with open(config_path, \'r\') as f:\r\n            config = yaml.safe_load(f)\r\n    except FileNotFoundError:\r\n        raise FileNotFoundError(f"Config not found: {config_path}")\r\n    except yaml.YAMLError as e:\r\n        raise ValueError(f"Invalid YAML in {config_path}: {e}")\r\n    \r\n    return config\r\n\r\ndef get_config_value(config: Dict, key_path: str, default=None) -> Any:\r\n    """Get nested config value safely."""\r\n    keys = key_path.split(\'.\')\r\n    value = config\r\n    for key in keys:\r\n        if isinstance(value, dict):\r\n            value = value.get(key)\r\n        else:\r\n            return default\r\n    return value if value is not None else default\r\n\r\n# Usage\r\nconfig = load_config(\'configs/default.yaml\')\r\nlearning_rate = get_config_value(config, \'training.learning_rate\', 0.001)\n'})}),"\n",(0,i.jsx)(r.h2,{id:"experiment-design",children:"Experiment Design"}),"\n",(0,i.jsx)(r.h3,{id:"clear-naming-convention",children:"Clear Naming Convention"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Good: Descriptive names\r\nwith mlflow.start_run(run_name="baseline_rf_100trees_depth10"):\r\n    pass\r\n\r\nwith mlflow.start_run(run_name="tuning_rf_grid_search_lr_range"):\r\n    pass\r\n\r\n# Bad: Unclear names\r\nwith mlflow.start_run(run_name="run_1"):\r\n    pass\r\n\r\nwith mlflow.start_run(run_name="test"):\r\n    pass\n'})}),"\n",(0,i.jsx)(r.h3,{id:"systematic-hyperparameter-tuning",children:"Systematic Hyperparameter Tuning"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\nfrom itertools import product\r\n\r\n# Define parameter ranges\r\nparam_grid = {\r\n    \'n_estimators\': [50, 100, 200],\r\n    \'max_depth\': [5, 10, 15],\r\n    \'learning_rate\': [0.01, 0.1]\r\n}\r\n\r\n# Generate combinations\r\nparam_combinations = [\r\n    dict(zip(param_grid.keys(), values))\r\n    for values in product(*param_grid.values())\r\n]\r\n\r\nmlflow.set_experiment("hyperparameter_tuning")\r\n\r\nbest_score = 0\r\nbest_params = None\r\n\r\n# Train all combinations\r\nfor params in param_combinations:\r\n    run_name = "_".join([f"{k}_{v}" for k, v in params.items()])\r\n    \r\n    with mlflow.start_run(run_name=run_name):\r\n        # Log all parameters\r\n        mlflow.log_params(params)\r\n        \r\n        # Train\r\n        model = train_model(params)\r\n        score = evaluate(model)\r\n        \r\n        # Log metrics\r\n        mlflow.log_metric("accuracy", score)\r\n        \r\n        # Track best\r\n        if score > best_score:\r\n            best_score = score\r\n            best_params = params\r\n\r\nprint(f"Best params: {best_params} with accuracy {best_score}")\n'})}),"\n",(0,i.jsx)(r.h2,{id:"reproducibility",children:"Reproducibility"}),"\n",(0,i.jsx)(r.h3,{id:"set-seeds",children:"Set Seeds"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import random\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef set_seed(seed: int):\r\n    """Set all random seeds for reproducibility."""\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n    tf.random.set_seed(seed)\r\n\r\n# Use in training\r\nSEED = 42\r\nset_seed(SEED)\r\n\r\nwith mlflow.start_run():\r\n    mlflow.log_param("random_seed", SEED)\r\n    # Training code\n'})}),"\n",(0,i.jsx)(r.h3,{id:"version-everything",children:"Version Everything"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\nfrom src import __version__\r\n\r\nwith mlflow.start_run():\r\n    # Code version\r\n    mlflow.set_tag("code_version", __version__)\r\n    \r\n    # Data version\r\n    mlflow.set_tag("data_version", "v2.0")\r\n    \r\n    # Library versions\r\n    mlflow.set_tag("sklearn_version", sklearn.__version__)\r\n    mlflow.set_tag("python_version", sys.version)\r\n    \r\n    # Git info (if using git)\r\n    import subprocess\r\n    git_hash = subprocess.check_output(\r\n        [\'git\', \'rev-parse\', \'HEAD\']\r\n    ).decode(\'utf-8\').strip()\r\n    mlflow.set_tag("git_commit", git_hash)\n'})}),"\n",(0,i.jsx)(r.h3,{id:"commit-before-running",children:"Commit Before Running"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:'# Ensure clean working directory\r\ngit status\r\n\r\n# Commit changes\r\ngit add -A\r\ngit commit -m "Add experiment configuration"\r\n\r\n# Then run experiment\r\npython src/train.py\n'})}),"\n",(0,i.jsx)(r.h2,{id:"collaboration",children:"Collaboration"}),"\n",(0,i.jsx)(r.h3,{id:"meaningful-tagging",children:"Meaningful Tagging"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\n\r\nwith mlflow.start_run():\r\n    mlflow.set_tags({\r\n        "author": "alice",\r\n        "team": "ml-research",\r\n        "project": "customer_churn",\r\n        "status": "completed",\r\n        "reviewed": False,\r\n        "production_ready": False\r\n    })\n'})}),"\n",(0,i.jsx)(r.h3,{id:"documentation",children:"Documentation"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\n\r\ndef train_model(config):\r\n    """\r\n    Train model with comprehensive logging.\r\n    \r\n    Args:\r\n        config: Dictionary with training configuration\r\n        \r\n    Returns:\r\n        tuple: (model, metrics)\r\n    \r\n    Logs to MLflow:\r\n        - All hyperparameters\r\n        - Training metrics (loss, accuracy)\r\n        - Validation metrics\r\n        - Model artifact\r\n        - Summary report\r\n    """\r\n    with mlflow.start_run():\r\n        mlflow.log_params(config)\r\n        \r\n        # Training logic...\r\n        model = train()\r\n        metrics = evaluate(model)\r\n        \r\n        mlflow.log_metrics(metrics)\r\n        mlflow.sklearn.log_model(model, "model")\r\n        \r\n        return model, metrics\n'})}),"\n",(0,i.jsx)(r.h3,{id:"share-results",children:"Share Results"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Create summary report\r\nsummary = f"""\r\n# Experiment Results\r\n\r\n## Configuration\r\n- Model: Random Forest\r\n- Parameters: {config}\r\n\r\n## Results\r\n- Accuracy: {accuracy:.4f}\r\n- Precision: {precision:.4f}\r\n- Recall: {recall:.4f}\r\n\r\n## Next Steps\r\n- Try gradient boosting\r\n- Increase data size\r\n- Feature engineering\r\n"""\r\n\r\n# Log to MLflow\r\nmlflow.log_text(summary, "summary.md")\n'})}),"\n",(0,i.jsx)(r.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(r.h3,{id:"efficient-logging",children:"Efficient Logging"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"import mlflow\r\nfrom collections import defaultdict\r\n\r\n# Don't log too frequently\r\n# \u274c Bad: Logs every iteration\r\nfor i in range(1000000):\r\n    mlflow.log_metric(\"loss\", loss, step=i)\r\n\r\n# \u2705 Good: Batch logging\r\nmetrics_buffer = defaultdict(list)\r\nfor i in range(1000000):\r\n    metrics_buffer['loss'].append(loss)\r\n    \r\n    # Flush every N steps\r\n    if i % 100 == 0:\r\n        for metric_name, values in metrics_buffer.items():\r\n            for step, value in enumerate(values, start=i-100):\r\n                mlflow.log_metric(metric_name, value, step=step)\r\n        metrics_buffer.clear()\n"})}),"\n",(0,i.jsx)(r.h3,{id:"memory-efficient-evaluation",children:"Memory Efficient Evaluation"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# Don\'t store all predictions\r\n# \u274c Bad: Loads entire dataset\r\nall_predictions = []\r\nfor batch in data_loader:\r\n    predictions = model.predict(batch)\r\n    all_predictions.extend(predictions)\r\naccuracy = calculate_accuracy(all_predictions, all_labels)\r\n\r\n# \u2705 Good: Running metrics\r\ntotal_correct = 0\r\ntotal_samples = 0\r\nfor batch in data_loader:\r\n    predictions = model.predict(batch)\r\n    total_correct += count_correct(predictions, batch.labels)\r\n    total_samples += len(batch)\r\n\r\naccuracy = total_correct / total_samples\r\nmlflow.log_metric("accuracy", accuracy)\n'})}),"\n",(0,i.jsx)(r.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,i.jsx)(r.h3,{id:"robust-training-loop",children:"Robust Training Loop"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import mlflow\r\nimport logging\r\n\r\nlogging.basicConfig(level=logging.INFO)\r\nlogger = logging.getLogger(__name__)\r\n\r\ndef safe_train(config: dict):\r\n    """Train with error handling and cleanup."""\r\n    with mlflow.start_run():\r\n        try:\r\n            logger.info("Starting training...")\r\n            mlflow.log_params(config)\r\n            \r\n            # Training code\r\n            model = train_model(config)\r\n            metrics = evaluate(model)\r\n            \r\n            mlflow.log_metrics(metrics)\r\n            mlflow.sklearn.log_model(model, "model")\r\n            \r\n            mlflow.set_tag("status", "completed")\r\n            logger.info("Training completed successfully")\r\n            \r\n        except ValueError as e:\r\n            logger.error(f"Invalid configuration: {e}")\r\n            mlflow.set_tag("status", "failed")\r\n            mlflow.set_tag("error_type", "validation_error")\r\n            raise\r\n            \r\n        except Exception as e:\r\n            logger.error(f"Unexpected error: {e}", exc_info=True)\r\n            mlflow.set_tag("status", "failed")\r\n            mlflow.set_tag("error_type", "unexpected_error")\r\n            raise\r\n            \r\n        finally:\r\n            # Cleanup\r\n            logger.info("Cleaning up resources")\n'})}),"\n",(0,i.jsx)(r.h2,{id:"testing",children:"Testing"}),"\n",(0,i.jsx)(r.h3,{id:"unit-tests",children:"Unit Tests"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'# tests/test_preprocessing.py\r\nimport unittest\r\nfrom src.preprocessing import normalize_data\r\n\r\nclass TestPreprocessing(unittest.TestCase):\r\n    def test_normalize_data(self):\r\n        """Test data normalization."""\r\n        data = [1, 2, 3, 4, 5]\r\n        normalized = normalize_data(data)\r\n        \r\n        self.assertAlmostEqual(normalized.mean(), 0)\r\n        self.assertAlmostEqual(normalized.std(), 1)\r\n\r\nif __name__ == \'__main__\':\r\n    unittest.main()\n'})}),"\n",(0,i.jsx)(r.p,{children:"Run tests:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Run all tests\r\npython -m pytest tests/\r\n\r\n# Run specific test\r\npython -m pytest tests/test_preprocessing.py::TestPreprocessing::test_normalize_data\r\n\r\n# With coverage\r\npip install pytest-cov\r\npytest --cov=src tests/\n"})}),"\n",(0,i.jsx)(r.h2,{id:"version-control",children:"Version Control"}),"\n",(0,i.jsx)(r.h3,{id:"commit-messages",children:"Commit Messages"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:'# \u274c Bad\r\ngit commit -m "updates"\r\n\r\n# \u2705 Good\r\ngit commit -m "Add RandomForest baseline with hyperparameter tuning\r\n\r\n- Implement baseline RF model\r\n- Add hyperparameter grid search\r\n- Log all metrics to MLflow\r\n- Achieve 94.3% accuracy on test set"\n'})}),"\n",(0,i.jsx)(r.h3,{id:"branch-strategy",children:"Branch Strategy"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:'# Main experiments\r\ngit branch feature/baseline-rf\r\ngit branch feature/hyperparameter-tuning\r\ngit branch feature/feature-engineering\r\n\r\n# Work on feature\r\ngit checkout feature/baseline-rf\r\ngit commit -m "Add RF baseline"\r\ngit push origin feature/baseline-rf\r\n\r\n# Create pull request for review\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.p,{children:"Following these practices will make your research more:"}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Reproducible"})," - Others can replicate your results"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Shareable"})," - Colleagues can understand your work"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Professional"})," - Ready for production deployment"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Efficient"})," - Minimal wasted effort and time"]}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:"Happy researching! \ud83d\ude80"})]})}function m(n={}){const{wrapper:r}={...(0,a.R)(),...n.components};return r?(0,i.jsx)(r,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:function(n,r,e){e.d(r,{R:function(){return s},x:function(){return o}});var t=e(6540);const i={},a=t.createContext(i);function s(n){const r=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(r):{...r,...n}},[r,n])}function o(n){let r;return r=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),t.createElement(a.Provider,{value:r},n.children)}}}]);